# =============================================================================
# vLLM Mistral-7B Deployment (Production Benchmark)
# =============================================================================
# Now that we've verified GPU access works with TinyLlama,
# deploying Mistral-7B with optimized settings for 20GB MIG partition
# =============================================================================
---
apiVersion: work.open-cluster-management.io/v1
kind: ManifestWork
metadata:
  name: llm-bench-vllm
  namespace: gpu1
  labels:
    app.kubernetes.io/part-of: llm-benchmark
spec:
  manifestConfigs:
    - resourceIdentifier:
        group: apps
        resource: deployments
        name: vllm-server
        namespace: llm-bench
      feedbackRules:
        - type: JSONPaths
          jsonPaths:
            - name: replicas
              path: .status.replicas
            - name: readyReplicas
              path: .status.readyReplicas
            - name: unavailableReplicas
              path: .status.unavailableReplicas
  workload:
    manifests:
      - apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: vllm-server
          namespace: llm-bench
          labels:
            app: vllm-server
            backend: vllm
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: vllm-server
          template:
            metadata:
              labels:
                app: vllm-server
                backend: vllm
            spec:
              serviceAccountName: bench-sa
              containers:
              - name: vllm
                image: vllm/vllm-openai:v0.6.4.post1
                command: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
                args:
                  - "--model"
                  - "mistralai/Mistral-7B-Instruct-v0.2"
                  - "--host"
                  - "0.0.0.0"
                  - "--port"
                  - "8000"
                  - "--tensor-parallel-size"
                  - "1"
                  - "--max-model-len"
                  - "2048"
                  - "--gpu-memory-utilization"
                  - "0.85"
                  - "--dtype"
                  - "half"
                  - "--trust-remote-code"
                  - "--enforce-eager"
                ports:
                - containerPort: 8000
                  name: http
                env:
                - name: HF_HOME
                  value: "/root/.cache/huggingface"
                - name: HF_HUB_OFFLINE
                  value: "0"
                - name: TRANSFORMERS_OFFLINE
                  value: "0"
                resources:
                  requests:
                    memory: "16Gi"
                    cpu: "4"
                    nvidia.com/gpu: "1"
                  limits:
                    memory: "24Gi"
                    cpu: "8"
                    nvidia.com/gpu: "1"
                volumeMounts:
                - name: dshm
                  mountPath: /dev/shm
                readinessProbe:
                  httpGet:
                    path: /health
                    port: 8000
                  initialDelaySeconds: 300
                  periodSeconds: 15
                livenessProbe:
                  httpGet:
                    path: /health
                    port: 8000
                  initialDelaySeconds: 600
                  periodSeconds: 30
                  failureThreshold: 10
              volumes:
              - name: dshm
                emptyDir:
                  medium: Memory
                  sizeLimit: "4Gi"
              tolerations:
              - key: nvidia.com/gpu
                operator: Exists
                effect: NoSchedule
              nodeSelector:
                nvidia.com/gpu.present: "true"
      - apiVersion: v1
        kind: Service
        metadata:
          name: vllm-service
          namespace: llm-bench
        spec:
          type: ClusterIP
          selector:
            app: vllm-server
          ports:
          - port: 8000
            targetPort: 8000
            name: http
      - apiVersion: route.openshift.io/v1
        kind: Route
        metadata:
          name: vllm-route
          namespace: llm-bench
        spec:
          to:
            kind: Service
            name: vllm-service
          port:
            targetPort: http
          tls:
            termination: edge
